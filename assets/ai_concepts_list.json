[
    {
        "title": "NLP",
        "descs": [
            "自然语言处理( Natural Language Processing, NLP)是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。自然语言处理是一门融语言学、计算机科学、数学于一体的科学。因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，所以它与语言学的研究有着密切的联系，但又有重要的区别。自然语言处理并不是一般地研究自然语言，而在于研制能有效地实现自然语言通信的计算机系统，特别是其中的软件系统。因而它是计算机科学的一部分",
            "自然语言处理主要应用于机器翻译、舆情监测、自动摘要、观点提取、文本分类、问题回答、文本语义对比、语音识别、中文OCR等方面",
            "知识体系",
            "<img src=\"https://picx.zhimg.com/70/v2-978a774f8be06e696e12ba6ef2287125_1440w.avis?source=172ae18b&biz_tag=Post\" class=\"img-thumbnail rounded\">"
        ]
    },
    {
        "title": "Transformer",
        "descs": [
            "Transformer是一个利用注意力机制来提高模型训练速度的模型。关于注意力机制可以参看[这篇文章](https://zhuanlan.zhihu.com/p/52119092)，trasnformer可以说是完全基于自注意力机制的一个深度学习模型，因为它适用于并行化计算，和它本身模型的复杂程度导致它在精度和性能上都要高于之前流行的RNN循环神经网络。",
            "你可以简单理解为它是一个黑盒子，当我们在做文本翻译任务是，我输入进去一个中文，经过这个黑盒子之后，输出来翻译过后的英文。",
            "<img class=\"img-thumbnail rounded\" src=\"https://pic4.zhimg.com/80/v2-1a4f5b236563d6307acb58cc5a95b2b7_1440w.webp\">",
            "里面主要有两部分组成：Encoder 和 Decoder",
            "<img src=\"https://pic2.zhimg.com/80/v2-8bf3b3ac8836ef1a9f16e1669fb29511_1440w.webp\" class=\"img-thumbnail rounded\">",
            "当我输入一个文本的时候，该文本数据会先经过一个叫Encoders的模块，对该文本进行编码，然后将编码后的数据再传入一个叫Decoders的模块进行解码，解码后就得到了翻译后的文本，对应的我们称Encoders为编码器，Decoders为解码器。",
            "那么编码器和解码器里边又都是些什么呢？",
            "细心的同学可能已经发现了，上图中的Decoders后边加了个s，那就代表有多个编码器了呗，没错，这个编码模块里边，有很多小的编码器，一般情况下，Encoders里边有6个小编码器，同样的，Decoders里边有6个小解码器。",
            "<img src=\"https://pic4.zhimg.com/80/v2-739d9498e0a36296240741be909d35f7_1440w.webp\" class=\"img-thumbnail rounded\">",
            "我们看到，在编码部分，每一个的小编码器的输入是前一个小编码器的输出，而每一个小解码器的输入不光是它的前一个解码器的输出，还包括了整个编码部分的输出。",
            "那么你可能又该问了，那每一个小编码器里边又是什么呢？",
            "我们放大一个encoder，发现里边的结构是一个自注意力机制加上一个前馈神经网络。",
            "<img src=\"https://pic1.zhimg.com/80/v2-8c63aaf7e71b94fdb5d6df89abdaf118_1440w.webp\" class=\"img-thumbnail rounded\">",
            "我们先来看下self-attention是什么样子的。",
            "我们通过几个步骤来解释：",
            "1、首先，self-attention的输入就是词向量，即整个模型的最初的输入是词向量的形式。那自注意力机制呢，顾名思义就是自己和自己计算一遍注意力，即对每一个输入的词向量，我们需要构建self-attention的输入。在这里，transformer首先将词向量乘上三个矩阵，得到三个新的向量，之所以乘上三个矩阵参数而不是直接用原本的词向量是因为这样增加更多的参数，提高模型效果。对于输入X1(机器)，乘上三个矩阵后分别得到Q1,K1,V1，同样的，对于输入X2(学习)，也乘上三个不同的矩阵得到Q2,K2,V2。",
            "<img src=\"https://pic3.zhimg.com/80/v2-15142b393f03a309c926754f00307d46_1440w.webp\" class=\"img-thumbnail rounded\">",
            "2、那接下来就要计算注意力得分了，这个得分是通过计算Q与各个单词的K向量的点积得到的。我们以X1为例，分别将Q1和K1、K2进行点积运算，假设分别得到得分112和96。",
            "<img src=\"https://pic2.zhimg.com/80/v2-42ccd93ac7540619b02ef03faef21c15_1440w.webp\" class=\"img-thumbnail rounded\">",
            "3、将得分分别除以一个特定数值8（K向量的维度的平方根，通常K向量的维度是64）这能让梯度更加稳定，则得到结果如下：",
            "<img src=\"https://pic3.zhimg.com/80/v2-8a98e66c20fb25e96e1f690309ae6166_1440w.webp\" class=\"img-thumbnail rounded\">",
            "4、将上述结果进行softmax运算得到，softmax主要将分数标准化，使他们都是正数并且加起来等于1。",
            "<img src=\"https://pic3.zhimg.com/80/v2-1701b674a3e09ae91301d6cd9727f912_1440w.webp\" class=\"img-thumbnail rounded\">",
            "5、将V向量乘上softmax的结果，这个思想主要是为了保持我们想要关注的单词的值不变，而掩盖掉那些不相关的单词（例如将他们乘上很小的数字）",
            "<img src=\"https://pic2.zhimg.com/80/v2-c18a30a6b8738af5cd1b5c0e2080e695_1440w.webp\" class=\"img-thumbnail rounded\">",
            "6、将带权重的各个V向量加起来，至此，产生在这个位置上（第一个单词）的self-attention层的输出，其余位置的self-attention输出也是同样的计算方式。",
            "<img src=\"https://pic3.zhimg.com/80/v2-3577071e71ccfa49a4f60f4a5187f0ce_1440w.webp\" class=\"img-thumbnail rounded\">",
            "将上述的过程总结为一个公式就可以用下图表示：",
            "<img src=\"https://pic4.zhimg.com/80/v2-0190eb46d1c46efc04926821e69fd377_1440w.webp\" class=\"img-thumbnail rounded\">",
            "self-attention层到这里就结束了吗？",
            "还没有，论文为了进一步细化自注意力机制层，增加了“多头注意力机制”的概念，这从两个方面提高了自注意力层的性能。",
            "第一个方面，他扩展了模型关注不同位置的能力，这对翻译一下句子特别有用，因为我们想知道“it”是指代的哪个单词。",
            "<img src=\"https://pic1.zhimg.com/80/v2-dc386abf38141384c43918689b0bbb64_1440w.webp\" class=\"img-thumbnail rounded\">",
            "第二个方面，他给了自注意力层多个“表示子空间”。对于多头自注意力机制，我们不止有一组Q/K/V权重矩阵，而是有多组（论文中使用8组），所以每个编码器/解码器使用8个“头”（可以理解为8个互不干扰自的注意力机制运算），每一组的Q/K/V都不相同。然后，得到8个不同的权重矩阵Z，每个权重矩阵被用来将输入向量投射到不同的表示子空间。",
            "经过多头注意力机制后，就会得到多个权重矩阵Z，我们将多个Z进行拼接就得到了self-attention层的输出：",
            "<img src=\"https://pic2.zhimg.com/80/v2-1be30f537678c89b2768ed31ff5bb491_1440w.webp\" class=\"img-thumbnail rounded\">",
            "上述我们经过了self-attention层，我们得到了self-attention的输出，self-attention的输出即是前馈神经网络层的输入，然后前馈神经网络的输入只需要一个矩阵就可以了，不需要八个矩阵，所以我们需要把这8个矩阵压缩成一个，我们怎么做呢？只需要把这些矩阵拼接起来然后用一个额外的权重矩阵与之相乘即可。",
            "<img src=\"https://pic4.zhimg.com/80/v2-7394f6eb418b403588b0ca5a6751749f_1440w.webp\" class=\"img-thumbnail rounded\">",
            "最终的Z就作为前馈神经网络的输入。",
            "接下来就进入了小编码器里边的前馈神经网模块了，关于前馈神经网络，网上已经有很多资料，在这里就不做过多讲解了，只需要知道，前馈神经网络的输入是self-attention的输出，即上图的Z,是一个矩阵，矩阵的维度是（序列长度×D词向量），之后前馈神经网络的输出也是同样的维度。",
            "以上就是一个小编码器的内部构造了，一个大的编码部分就是将这个过程重复了6次，最终得到整个编码部分的输出。",
            "然后再transformer中使用了6个encoder，为了解决梯度消失的问题，在Encoders和Decoder中都是用了残差神经网络的结构，即每一个前馈神经网络的输入不光包含上述self-attention的输出Z，还包含最原始的输入。",
            "上述说到的encoder是对输入（机器学习）进行编码，使用的是自注意力机制+前馈神经网络的结构，同样的，在decoder中使用的也是同样的结构。也是首先对输出（machine learning）计算自注意力得分，不同的地方在于，进行过自注意力机制后，将self-attention的输出再与Decoders模块的输出计算一遍注意力机制得分，之后，再进入前馈神经网络模块。",
            "<img src=\"https://pic4.zhimg.com/80/v2-5e32534b9a651289cb3eb2b409d5996b_1440w.webp\" class=\"img-thumbnail rounded\">",
            "以上，就讲完了Transformer编码和解码两大模块，那么我们回归最初的问题，将“机器学习”翻译成“machine learing”，解码器输出本来是一个浮点型的向量，怎么转化成“machine learing”这两个词呢？",
            "是个工作是最后的线性层接上一个softmax，其中线性层是一个简单的全连接神经网络，它将解码器产生的向量投影到一个更高维度的向量（logits）上，假设我们模型的词汇表是10000个词，那么logits就有10000个维度，每个维度对应一个惟一的词的得分。之后的softmax层将这些分数转换为概率。选择概率最大的维度，并对应地生成与之关联的单词作为此时间步的输出就是最终的输出啦！！",
            "假设词汇表维度是6，那么输出最大概率词汇的过程如下：",
            "<img src=\"https://pic4.zhimg.com/80/v2-6d0a0d38ab824914942121d1ae78cd0b_1440w.webp\" class=\"img-thumbnail rounded\">",
            "以上就是Transformer的框架了，但是还有最后一个问题，我们都是到RNN中的每个输入是时序的，是又先后顺序的，但是Transformer整个框架下来并没有考虑顺序信息，这就需要提到另一个概念了：“位置编码”。",
            "Transformer中确实没有考虑顺序信息，那怎么办呢，我们可以在输入中做手脚，把输入变得有位置信息不就行了，那怎么把词向量输入变成携带位置信息的输入呢？",
            "我们可以给每个词向量加上一个有顺序特征的向量，发现sin和cos函数能够很好的表达这种特征，所以通常位置向量用以下公式来表示：",
            "<img src=\"https://pic1.zhimg.com/80/v2-a671b951ef42d09c349db12c35175998_1440w.webp\" class=\"img-thumbnail rounded\">",
            "<img src=\"https://pic1.zhimg.com/80/v2-c17ebc4594bd0c0d01fab289abde5ec4_1440w.webp\" class=\"img-thumbnail rounded\">"
        ]
    },
    {
        "title": "LLM",
        "descs": [
            "Large Language Model (LLM) 即大规模语言模型，是一种基于深度学习的自然语言处理模型，它能够学习到自然语言的语法和语义，从而可以生成人类可读的文本。所谓\"语言模型\"，就是只用来处理语言文字（或者符号体系）的 AI 模型，发现其中的规律，可以根据提示 (prompt)，自动生成符合这些规律的内容。LLM 通常基于神经网络模型，使用大规模的语料库进行训练，比如使用互联网上的海量文本数据。这些模型通常拥有数十亿到数万亿个参数，能够处理各种自然语言处理任务，如自然语言生成、文本分类、文本摘要、机器翻译、语音识别等。"
        ]
    },
    {
        "title": "LLaMA",
        "descs": [
            "LLaMA（Large Language Model Meta AI）是近期Meta发布的一种大型语言模型，和GPT一样都是由Transformer的解码器组成，在生成文本、进行对话、总结书面材料等复杂的任务方面表现出了巨大的潜力。其数据集来源都是公开数据集，无任何定制数据集，保证了其工作与开源兼容和可复现，整个训练数据集在 token 化之后大约包含 1.4T 的 token。",
            "LLaMA有4种不同的参数规格: 70亿、130亿、330亿和650亿参数,远小于GPT3参数量(1700亿)。"
        ]
    },
    {
        "title": "LSTM",
        "descs": [
            "LSTM（Long Short-Term Memory）是一种长短期记忆网络，是一种特殊的RNN（循环神经网络）。与传统的RNN相比，LSTM更加适用于处理和预测时间序列中间隔较长的重要事件。",
            "传统的RNN结构可以看做是多个重复的神经元构成的“回路”，每个神经元都接受输入信息并产生输出，然后将输出再次作为下一个神经元的输入，依次传递下去。这种结构能够在序列数据上学习短时依赖关系，但是由于梯度消失和梯度爆炸问题，RNN在处理长序列时难以达到很好的性能。而LSTM通过引入记忆细胞、输入门、输出门和遗忘门的概念，能够有效地解决长序列问题。记忆细胞负责保存重要信息，输入门决定要不要将当前输入信息写入记忆细胞，遗忘门决定要不要遗忘记忆细胞中的信息，输出门决定要不要将记忆细胞的信息作为当前的输出。这些门的控制能够有效地捕捉序列中重要的长时间依赖性，并且能够解决梯度问题。",
            "LSTM长短期神经网络的主要用途包括：",
            "文本生成：LSTM可以学习语言模型，在给定一些前缀的情况下生成合理的文本。",
            "语音识别：LSTM可以用来识别语音，可以识别说话人的声音，也可以用来识别说话的语音。",
            "机器翻译： LSTM可以用来翻译文本，可以把一种语言翻译成另一种语言。",
            "时间序列预测： LSTM可以用来预测时间序列数据，比如股票价格，天气预报等。",
            "除了文本生成，语音识别和机器翻译这些领域之外，LSTM也在其他领域有所应用。比如说在生物信息学领域，LSTM可以用来预测RNA结构，在金融领域，LSTM可以用来分析股票价格和货币汇率。",
            "举一个简单的预测金融产品价格的实例：",
            "假设我们有过去5年中某金融产品每天的收盘价格数据，我们可以使用这些历史数据来训练一个LSTM模型。 首先，我们需要将这些历史数据按照时间顺序排列，并将其分为训练集和测试集。我们可以使用前80%的数据来训练LSTM模型，剩下20%的数据来评估模型的性能。 接下来，我们就可以使用这个LSTM模型对未来的金融产品价格进行预测。我们可以输入过去一段时间的价格数据（比如过去10天的价格），LSTM模型会输出预测的下一天的价格。 我们也可以预测出未来几天或几周的股票价格走势，根据预测值对投资策略进行调整。",
            "这只是一个非常简单的例子，在实际应用中，还需要考虑更多因素，如数据预处理，特征工程，模型调参等步骤。 另外需要注意的是，机器学习模型的预测只是根据历史数据做出的统计预测，不能保证结果一定准确，对于金融市场的预测更是如此。",
            "总的来说，LSTM是一种非常强大的时间序列处理网络，能够解决循环问题和长时间依赖性的问题，在自然语言处理，语音识别，机器翻译等领域都有着广泛的应用。",
            "记忆细胞（memory cell）: 它是 LSTM 的核心，负责保存重要的信息，并将这些信息传递给后面的网络层。",
            "输入门（input gate）: 决定了当前输入信息是否写入记忆细胞，也就是说，能够控制输入信息对记忆细胞的影响。",
            "遗忘门（forget gate）: 决定了记忆细胞中的信息是否被遗忘，也就是说，能够控制记忆细胞中保存的信息会不会消失。",
            "输出门（output gate）: 决定了记忆细胞中的信息是否输出，也就是说，能够控制记忆细胞中保存的信息会不会对后面的网络层造成影响。",
            "这四个部分通过计算权重矩阵和输入信号的点积，并通过激活函数（通常是sigmoid函数）计算出每个门的输出值，再乘上记忆细胞的值来进行最终计算。",
            "在训练 LSTM 模型时，我们通过反向传播算法来学习 LSTM 网络中的权重参数，并不断迭代来提高模型的性能。"
        ]
    },
    {
        "title": "Bard",
        "descs": [
            "Google Bard 是一种新的聊天机器人工具，是在围绕生成式人工智能（如ChatGPT）的所有热议之后推出的。Bard 旨在模拟与人的对话，并结合使用自然语言处理和机器学习，为您可能提出的问题提供逼真且有用的回答。",
            "Google Bard 是 Google 对 ChatGPT 的回应。它是一个具有许多相同功能的 AI 聊天机器人，但旨在最终增强谷歌自己的搜索工具（就像Bing Chat现在使用GPT-4的方式），并为企业提供自动化支持和类似人类的交互。",
            "它已经开发多年，并使用 LaMDA（对话应用程序语言模型）技术。它建立在谷歌的 Transformer 神经网络架构之上，该架构也是其他人工智能生成工具的基础，例如 ChatGPT 的 GPT-3.5 语言模型。",
            "Google Bard 和 ChatGPT 都使用自然语言模型和机器学习来创建他们的聊天机器人，但每个都有不同的功能集。在撰写本文时，ChatGPT 完全基于 2021 年之前收集的大部分数据，而 Google Bard 有可能使用最新信息进行响应。ChatGPT 主要专注于对话式问答，但它现在也用于 Bing 的搜索结果中，以回答更多对话式搜索。Google Bard 将以同样的方式使用，但专门用于增强 Google。",
            "目前最大的障碍是 Bard 只会响应特定的提示，因此您经常需要多次请求它来生成代码、翻译语言以及它可以在生成文本之外执行的许多其他功能。"
        ]
    },
    {
        "title": "BERT",
        "descs": [
            "BERT是2018年10月由Google AI研究院提出的一种预训练模型。BERT的全称是Bidirectional Encoder Representation from Transformers。BERT在机器阅读理解顶级水平测试SQuAD1.1中表现出惊人的成绩: **全部两个衡量指标上全面超越人类，并且在11种不同NLP测试中创出SOTA表现，包括将GLUE基准推高至80.4% (绝对改进7.6%)，MultiNLI准确度达到86.7% (绝对改进5.6%)，成为NLP发展史上的里程碑式的模型成就。",
            "BERT全称为Bidirectional Encoder Representation from Transformers，是一种用于语言表征的预训练模型。它强调不再像以往传统单向模型预训练的方式，而是采用新的训练策略，以致能生成深度双向语言表征模型。",
            "为何使用BERT",
            "在NLP算法落地过程中遇到最大挑战通常是缺乏足够训练数据。总体而言，获取大量文本并非难事，但帮助算法清楚问题定义需要有足够数量被标记样本。对文本进行人工标注是一件耗时耗力的工作，并且因为每个人对文本内容理解层次不同，产生的标注带有主观倾向也会造成偏差。",
            "为了弥合数据鸿沟，多种仅使用未标记文本语料的语言模型被相继提出，这种模型被称为预训练模型（Pre-trained Models, PTM），BERT就是这种技术开花结的最好的果之一。预训练模型是通过自监督学习从大规模数据中得到、与具体业务无关的模型。如哈工大开源的中文预训练模型BERT-wwm就是在百科、新闻、问答等数据上训练得到，这些数据含有的词汇量达到5.4亿之多。",
            "BERT如何使用",
            "在NLP中，预训练模型的训练被称为是上游任务；而具体业务，如：情感分析、阅读理解、文本摘要等则被称作是下游任务。通过对预训练模型在相对少量业务数据上进行训练，便可将模型用于不同目的的下游任务，这个过程称为微调（Fine-Tune）。微调可显著提高模型准确性。此外，与重新开始对业务数据进行训练相比，微调仅需要很少样本就可以让模型拥有良好性能。",
            "BERT的原理",
            "上游进行语言模型的预训练，下游微调并应用到具体业务中，这种模式被称为迁移学习（Transfer Learning）。在架构方面，BERT使用大量迁移模型Transformer中的编码器，并对输入文本进行位置编码，结合BERT独特的训练策略来得到预训练模型。",
            "1、迁移学习",
            "Transformer是谷歌于2017年年底在论文《Attention is all you need》中提出的一种序列到序列（Seq2seq）模型，它主要由编码器（Encoder）和解码器（Decoder）两部分组成，详见图3。",
            "介绍Transformer模型最好例子就是它在机器翻译中的应用。在英文翻译成中文过程中，编码器负责阅读与学习输入的英文文本，通过捕捉载体所包含信息学会一定的语言概念，这被称为是上下文（Context）。上下文信息本质是语义在向量空间的一种映射，即语义的数学化表达，一个著名例子就是：国王-男性+女性=女王。之后，编码器将所学内容以隐藏层（Hidden Layer）形式传递给解码器，解码器再利用这些知识进行文本翻译工作。具体来说，在生成中文文本过程中，解码器会对当前中文单词根据上下文信息来预测下一个中文单词，之后再根据下一个词预测下一个词的下一个词，循环往复，直至生成完整句子，这种做法也体现了序列模型的特性。",
            "由于BERT目标是生成用于语言表示的预训练模型，因此只需要编码器部分即可。所学到上下文信息在BERT中以768维度向量存储。",
            "<img class=\"img-thumbnail rounded\" src=\"https://pic4.zhimg.com/80/v2-25310f0fc1da28600df272aaf8e4ce03_720w.webp\">",
            "2、位置嵌入",
            "在序列模型中，词在句中位置对语料学习尤为重要。在训练文本时，长短时记忆网络（Long Short Term Memory Network, LSTM）通过一定的顺序读取文本：从左向右或是从右向左，以此来得到词在文中的位置信息，这样的模型称为单向模型。",
            "与单向模型不同的是，在词向量（Word Embedding）进入编码器之前，Transformer模型使用位置编码（Positional Encoding）来提供语序信息。这种设计通过为词向量加入独一无二的纹理信息来表征词在句中的位置，纹理信息则通过sin函数与cos函数的线性变换（公式1-2）生成。因此，编码器可以一次读取整个文本序列，这样的语境化特性使Transformer可以基于词的所有周围环境来学习上下文，并且可以接收更庞大的数据量。",
            "3、训练策略",
            "BERT在模型的训练过程中，会同时结合以下两种策略：",
            "遮蔽预测（Masked LM）：BERT会随机遮蔽掉句中部分词，然后通过未遮蔽掉的词提供上下文来预测这些被遮蔽的词是什么。这种训练模式在以往单向模型中很难实现，这意味着与单向模型相比，BERT对上下文有着更深刻的感知。",
            "下一句预测（Next Sentence Prediction）：为了让模型能够预测两个给定句子在顺序上是否有逻辑关系，在BERT的训练过程中，模型接收成对的句子作为输入，并预测第二句是否是第一句的后续。通过这样的训练，模型不仅能学习句内信息，还能清楚地捕捉到句间逻辑。这种独特的学习模式也使其在问答系统、阅读理解等问题上有出色的发挥。",
            "毫无疑问，BERT在使用机器学习进行自然语言处理方面取得巨大技术突破。谷歌团队也在GitHub上开源了BERT源代码与预训练模型，涵盖103种语言，可以轻松地使用开源预训练模型进行下游任务训练也让它有更广泛的应用。",
            "Pre-training 的主体结构",
            "<img class=\"img-thumbnail rounded\" src=\"https://pic3.zhimg.com/80/v2-f0618dc2c2f62bd8d71c2195947be1d6_1440w.webp\">",
            "token表征的组成",
            "<img class=\"img-thumbnail rounded\" src=\"https://pic1.zhimg.com/80/v2-ee823df66560850baa34128af76a6334_1440w.webp\">"
        ]
    },
    {
        "title": "ChatGPT",
        "descs": [
            "ChatGPT是美国人工智能研究实验室OpenAI新推出的一种人工智能技术驱动的自然语言处理工具，使用了Transformer神经网络架构，也是GPT-3.5架构，这是一种用于处理序列数据的模型，拥有语言理解和文本生成能力，尤其是它会通过连接大量的语料库来训练模型，这些语料库包含了真实世界中的对话，使得ChatGPT具备上知天文下知地理，还能根据聊天的上下文进行互动的能力，做到与真正人类几乎无异的聊天场景进行交流。ChatGPT不单是聊天机器人，还能进行撰写邮件、视频脚本、文案、翻译、代码等任务。",
            "ChatGPT受到关注的重要原因是引入新技术RLHF(Reinforcement Learning with Human Feedback，即基于人类反馈的强化学习)。RLHF解决了生成模型的一个核心问题，即如何让人工智能模型的产出和人类的常识、认知、需求、价值观保持一致。ChatGPT是AIGC（AI-Generated Content，人工智能生成内容）技术进展的成果。该模型能够促进利用人工智能进行内容创作、提升内容生产效率与丰富度。",
            "ChatGPT的使用上还有局限性，模型仍有优化空间。ChatGPT模型的能力上限是由奖励模型决定，该模型需要巨量的语料来拟合真实世界，对标注员的工作量以及综合素质要求较高。ChatGPT可能会出现创造不存在的知识，或者主观猜测提问者的意图等问题，模型的优化将是一个持续的过程。若AI技术迭代不及预期，NLP模型优化受限，则相关产业发展进度会受到影响。此外，ChatGPT盈利模式尚处于探索阶段，后续商业化落地进展有待观察。"
        ]
    },
    {
        "title": "AIGC",
        "descs": [
            "生成式人工智能(Artificial Intelligence Generated Content)",
            "AIGC能够生成文本、图像、视频、音频等多种形式的内容。随着技术的发展，AIGC已经开始被广泛应用于新闻、娱乐、教育、医疗、金融和广告等领域，其应用范围还在不断扩大。",
            "通过单个大规模数据的学习训练，令AI具备了多个不同领域的知识，只需要对模型进行适当的调整修正，就能完成真实场景的任务。",
            "短期来看AIGC改变了基础的生产力工具，中期来看会改变社会的生产关系，长期来看促使整个社会生产力发生质的突破，在这样的生产力工具、生产关系、生产力变革中，生产要素——数据价值被极度放大。AIGC把数据要素提到时代核心资源的位置，在一定程度上加快了整个社会的数字化转型进程。",
            "AIGC技术还可以用于**多语言翻译、智能客服、虚拟人物、人机交互**等领域，这些应用都具有非常广泛的前景和市场潜力。",
            "- 首先，AIGC技术生成的内容可能存在错误或不准确的问题，这可能会导致信息的误传或其他风险。",
            "- 其次，AIGC技术也可能被用于伪造、抄袭或侵权等不当行为，从而损害他人利益。此外，保护用户隐私和数据安全也是需要考虑的问题。"
        ]
    },
    {
        "title": "Hugging Face",
        "descs": [
            "Hugging Face是一家开源模型库公司。",
            "http://www.huggingface.co",
            "提供的资源",
            "1. Datasets：数据集，以及数据集的下载地址",
            "2. Models：各个预训练模型",
            "3. course：免费的nlp课程，可惜都是英文的",
            "4. docs：文档",
            "2016年，Hugging Face还只是一个AI聊天机器人，没想到过了7年后，摇身一变成为AI技术开源平台，让用户分享、发布预训练模型与自然语言处理、影像辨识资料集等等，就像是一个大家相互讨论与共享的AI社群，不仅估值冲破20亿美元，还是Github上成长最迅速的AI专案。近期也发布HuggingGPT，让开发者透过ChatGPT在Hugging Face找到合适的训练模型。"
        ]
    },
    {
        "title": "文心一言",
        "descs": [
            "文心一言（英文名：ERNIE Bot）是百度全新一代知识增强大语言模型，文心大模型家族的新成员，能够与人对话互动，回答问题，协助创作，高效便捷地帮助人们获取信息、知识和灵感。文心一言是知识增强的大语言模型，基于飞桨深度学习平台和文心知识增强大模型，持续从海量数据和大规模知识中融合学习具备知识增强、检索增强和对话增强的技术特色",
            "文心一言有五大能力，文学创作、商业文案创作、数理逻辑推算、中文理解、多模态生成。"
        ]
    },
    {
        "title": "prompt",
        "descs": [
            "prompt是人工智能（AI）提示词，是一种利用自然语言来指导或激发人工智能模型完成特定任务的方法。它是近年来在自然语言处理（NLP）领域兴起的一种新范式，也被称为“预训练-提示-预测”（pretrain-prompt-predict）。prompt的作用是给AI模型提示输入信息的上下文和输入模型的参数信息。训练有监督学习或者无监督学习的模型时，prompt可以帮助模型更好地理解输入的意图，并作出相应的响应。此外，prompt还可以帮助提高模型的可解释性和可访问性。",
            "人工智能提示词的核心思想是，利用预训练模型已经学习到的大量知识和语言规律，通过设计合适的文本形式和内容，将下游任务的输入输出转化为预训练模型期望和擅长的形式，从而达到最佳的效果。例如，如果要让GPT-3进行情感分类任务，可以将输入句子后面加上一个模板，如“这句话表达了一个“X”的情感”，然后给出两个选项，如“正面”或“负面”，让模型预测“X”的值。这样就相当于将情感分类任务转化为了完形填空任务，而完形填空任务是GPT-3所擅长的。人工智能提示词的优点是显而易见的：只要找到一个合适的提示词，就可以利用一个预训练模型完成多种任务，不需要对每个特定任务进行训练和微调。这样可以节省大量的时间、资源和数据，并且可以提升小样本学习甚至零样本学习的性能。",
            "AI提示词（Prompt）是一种利用自然语言来指导或激发人工智能模型完成特定任务的方法。它是近年来在自然语言处理（NLP）领域兴起的一种新范式，也被称为“预训练-提示-预测”（pretrain-prompt-predict）。",
            "AI指令编写的方法，编写AI提示词（Prompt）的一般步骤如下：",
            "① 明确任务描述：在提交Prompt时，应该尽可能清晰明确地描述任务的具体信息，包括任务目标、所需操作、相关条件等。例如，在与AI聊天机器人交互时，可以通过简单的问题来引导机器理解用户需求。如：“你需要我帮你做什么？” “你需要查询什么信息？” 能够尽量明确任务描述，让机器能够更好地理解用户需求并给出相应建议。",
            "② 使用常用词汇：在提交Prompt时，应使用常见的词汇和语言表达方式，避免使用生僻单词和复杂的句式，以便机器更容易理解。",
            "③ 考虑语境和上下文：在提交Prompt时，应该考虑到所处的语境和上下文环境，以便机器能够从语言环境中获取更多的信息来理解用户的意图。",
            "④ 编写Prompt：根据任务描述，选择合适的词汇和语言表达方式，编写AI提示词（Prompt）。AI提示词（Prompt）应该简洁明了，易于理解，并且与任务描述相关。",
            "AI指令学习的重点与意义，AI提示词（Prompt）的学习重点与意义可以从以下几个方面来阐述：",
            "① 提高效率：AI提示词（Prompt）可以帮助人工智能模型更快速地理解用户意图，提高回答问题和执行任务的效率。",
            "② 降低错误率：AI提示词（Prompt）是一种有监督的学习方法，可以通过回答用户问题来不断改进和优化模型的表现，从而降低错误率。",
            "③ 提高可解释性：AI提示词（Prompt）可以让模型更好地理解输入的意图，从而提高模型的可解释性和可访问性。",
            "④ 节约人力成本和时间成本：使用AI提示词（Prompt）可以避免人工编写Prompt的繁琐过程，节约人力成本和时间成本。",
            "⑤ 增强模型的泛化能力：使用AI提示词（Prompt）可以让模型更好地适应不同的输入，增强模型的泛化能力。",
            "人工智能提示词的缺点也是存在的：",
            "首先，设计一个合适的提示词并不容易，需要有一定的技巧和艺术感，而且对于不同的模型、任务和数据可能需要不同的提示词。",
            "其次，提示词对于模型性能的影响很大，一点微小的差别可能导致结果完全不同。",
            "第三，提示词可能会引入一些偏见或误导，例如如果给出了错误或不恰当的提示信息或选项，可能会让模型产生错误或不恰当的输出。"
        ]
    },
    {
        "title": "fine-tuning",
        "descs": [
            "迁移学习(Transfer learning) 顾名思义就是把已训练好的模型（预训练模型）参数迁移到新的模型来帮助新模型训练。考虑到大部分数据或任务都是存在相关性的，所以通过迁移学习我们可以将已经学到的模型参数（也可理解为模型学到的知识）通过某种方式来分享给新模型从而加快并优化模型的学习效率不用像大多数网络那样从零学习。",
            "其中，实现迁移学习有以下三种手段：",
            "1. Transfer Learning：冻结预训练模型的全部卷积层，只训练自己定制的全连接层。",
            "2. Extract Feature Vector：先计算出预训练模型的卷积层对所有训练和测试数据的特征向量，然后抛开预训练模型，只训练自己定制的简配版全连接网络。",
            "3. Fine-tuning：冻结预训练模型的部分卷积层（通常是靠近输入的多数卷积层，因为这些层保留了大量底层信息）甚至不冻结任何网络层，训练剩下的卷积层（通常是靠近输出的部分卷积层）和全连接层。",
            "Fine-tuning原理",
            "Fine-tuning的原理就是利用已知的网络结构和已知的网络参数，修改output层为我们自己的层，微调最后一层前的若干层的参数，这样就有效利用了深度神经网络强大的泛化能力，又免去了设计复杂的模型以及耗时良久的训练，所以fine tuning是当数据量不足时的一个比较合适的选择。",
            "Fine-tuning 意义",
            "1. 站在巨人的肩膀上：前人花很大精力训练出来的模型在大概率上会比你自己从零开始搭的模型要强悍，没有必要重复造轮子。",
            "2. 训练成本可以很低：如果采用导出特征向量的方法进行迁移学习，后期的训练成本非常低，用CPU都完全无压力，没有深度学习机器也可以做。",
            "3. 适用于小数据集：对于数据集本身很小（几千张图片）的情况，从头开始训练具有几千万参数的大型神经网络是不现实的，因为越大的模型对数据量的要求越大，过拟合无法避免。这时候如果还想用上大型神经网络的超强特征提取能力，只能靠迁移学习。"
        ]
    },
    {
        "title": "GPT",
        "descs": [
            "随着人工智能技术的不断发展，自然语言处理技术也变得越来越重要。而（Generative Pre-trained Transformer）正是一种基于深度学习技术的自然语言处理技术，它被广泛应用于语言生成、问答系统、机器翻译和文本分类等领域。",
            "GPT是一种基于Transformer模型的深度学习技术，它的基本原理是使用大规模文本数据进行预训练，然后再针对具体任务进行微调，从而实现更加精准的文本生成和处理。具体来说，GPT使用多层Transformer模型来对文本进行编码和解码，其中编码器将输入文本转换为表示，而解码器则使用该表示来生成输出文本。GPT模型的训练数据通常是从互联网上爬取的海量文本数据，例如维基百科、新闻报道和社交媒体内容等，这些数据被用于训练GPT模型的预测和生成能力。",
            "GPT的应用非常广泛，其中最为著名的应用之一就是语言生成。通过使用GPT模型，我们可以生成一些自然语言文本，例如文章、故事、对话等等。此外，GPT还被广泛运用于问答系统和机器翻译等领域，它可以帮助我们更好地理解和处理自然语言信息。",
            "GPT 的优势：",
            "首先，GPT可以利用海量的预训练数据进行训练，这可以帮助模型更好地理解语言规则和语义关系，从而提高模型的性能。",
            "其次，GPT的模型结构非常灵活，可以根据不同的任务进行微调，这使得GPT在各种自然语言处理任务中表现出色。",
            "最后，GPT生成的文本相对于其他技术生成的文本更加自然、流畅和连贯，这使得它在文本生成任务中具有巨大优势。",
            "GPT面临的挑战：",
            "最大的挑战之一就是如何减少模型的体积和计算量，以便在移动设备和低功耗设备上运行。为了解决这个问题，研究人员一直在尝试各种不同的方法，例如对模型进行剪枝、量化和压缩等。另外，GPT在处理一些特定领域的问题时可能会出现一些问题，例如在处理科技、金融和医疗等领域的专业术语时，模型的表现可能不如人类专家。为了解决这个问题，研究人员正在尝试将领域知识和专家知识融入到模型中，以提高模型的性能。"
        ]
    }
]